import streamlit as st
from service.constant import  ROLE_TYPE, FUNC_TYPE
from service.display import print_message, print_history_message
from service.history import init_history
from service.utils import handle_message, is_txt_file
from service.input import get_prompt
from LLM.llm import load_vector_store, search_similarity, get_answer
from langchain_groq import ChatGroq
from dotenv import load_dotenv
from service.tavily_tool import tavily_search_korean
from langchain.tools import Tool
import os

st.title("ì‚¼ì„±ì „ì ì·¨ì—… ì»¨ì„¤íŒ… ì±—ë´‡")

load_dotenv()
init_history()
vector_db = load_vector_store() 
llm = ChatGroq(model="gemma2-9b-it")

# api í‚¤ ì„¤ì •
TAVILY_API_KEY = os.environ.get("TAVILY_API_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# ì„ íƒì§€ ì •ì˜
option = st.selectbox(
    "ê¸°ëŠ¥ì„ ì„ íƒí•˜ì„¸ìš” ğŸ‘‡",
    ["ìì†Œì„œ í”¼ë“œë°±", "êµ¬ì¸ì˜ë¢° ê²€ìƒ‰"]
)

# ì¡°ê±´ ë¶„ê¸°
if option == "ìì†Œì„œ í”¼ë“œë°±":
    print_message(ROLE_TYPE.assistant.name,"ì•ˆë…•í•˜ì„¸ìš”! ì‚¼ì„±ì „ì ì·¨ì—… ì»¨ì„¤íŒ… ì±—ë´‡ì…ë‹ˆë‹¤.")
    print_message(ROLE_TYPE.assistant.name,"ğŸ“œ ìì†Œì„œ í”¼ë“œë°± í˜ì´ì§€ì…ë‹ˆë‹¤. \
                    ìì†Œì„œ íŒŒì¼ì„ ì˜¬ë ¤ì£¼ì‹œê³ , ì›í•˜ì‹œëŠ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")
    prompt = get_prompt(is_file=True)
    if prompt is not None : 
        # ì‚¬ìš©ì ì…ë ¥/ ìì†Œì„œ ì§ˆë¬¸, ë‹µë³€ì„ êµ¬ë¶„ì§“ê¸° ìœ„í•´ msg_typeì„ ì¶”ê°€í•¨.
        # accept_fileì´ ìˆê¸° ë•Œë¬¸ì— chatinputValue ê°ì²´ê°€ ë˜ë‹ˆ .textë¥¼ ë¶™ì—¬ì•¼ë¨.
        print_history_message(FUNC_TYPE.cover_letter.name)
        handle_message(ROLE_TYPE.user, prompt.text, FUNC_TYPE.cover_letter.name)
        if not prompt.files : 
            handle_message(ROLE_TYPE.assistant, 
            "ğŸš¨ ì˜¤ë¥˜ : íŒŒì¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!!",FUNC_TYPE.cover_letter.name,is_streaming=True)
        else : 
            file = is_txt_file(prompt.files[0])
            messages = file.read().decode("utf-8")
            context = search_similarity(messages, vector_db)
            response =  get_answer(llm, context, messages, prompt.text)
            handle_message(ROLE_TYPE.assistant,response,FUNC_TYPE.cover_letter.name, is_streaming=True)
            
elif option == "êµ¬ì¸ì˜ë¢° ê²€ìƒ‰":
    print_message(ROLE_TYPE.assistant.name,"ì•ˆë…•í•˜ì„¸ìš”! ì‚¼ì„±ì „ì ì·¨ì—… ì»¨ì„¤íŒ… ì±—ë´‡ì…ë‹ˆë‹¤.")
    print_message(ROLE_TYPE.assistant.name,
    "ğŸ¢ êµ¬ì¸ì˜ë¢° ê²€ìƒ‰ í˜ì´ì§€ì…ë‹ˆë‹¤. ì‚¼ì„±ì „ì ì±„ìš©, ë‰´ìŠ¤, íŠ¸ë Œë“œ ê´€ë ¨ ì§ˆë¬¸ì— ì‚¬ìš©í•´ì£¼ì„¸ìš”~")
    prompt = get_prompt(is_file=False)
    if prompt is not None : 
        print_history_message(FUNC_TYPE.job_search.name)
        handle_message(ROLE_TYPE.user, prompt,FUNC_TYPE.job_search.name)

        # âœ… LangChain Tool ê°ì²´ ìƒì„±
        tavily_tool = Tool(
            name="TavilySearchKorean",
            func=tavily_search_korean,
            description=(
                "ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•œê¸€ë¡œ ìš”ì•½í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. "
                "ì‚¼ì„±ì „ì ì±„ìš©, ë‰´ìŠ¤, íŠ¸ë Œë“œ ê´€ë ¨ ì§ˆë¬¸ì— ì‚¬ìš©í•˜ì„¸ìš”."
            )
        )
        result = tavily_search_korean(prompt, tavily_key=TAVILY_API_KEY, openai_key=OPENAI_API_KEY, llm=llm)
        handle_message(ROLE_TYPE.assistant, "ğŸ” ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (í•œêµ­ì–´): " + result, FUNC_TYPE.job_search.name ,is_streaming=True)
    





