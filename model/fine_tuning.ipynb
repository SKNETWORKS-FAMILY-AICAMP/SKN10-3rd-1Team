{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install typing_extensions\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "    r = 16,           # 0보다 큰 어떤 숫자도 선택 가능! 8, 16, 32, 64, 128이 권장됩니다.\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",   # 바이어스를 지원합니다.\n",
    "    random_state = 3407\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# JSON 파일 읽기\n",
    "def load_and_preprocess_data(file_path='train.json'):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 데이터 플랫화 및 정제\n",
    "    processed_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        for review in entry['reviews']:\n",
    "            # 불필요한 특수문자 및 따옴표 제거\n",
    "            question = review['question'].strip().replace(\"'\", \"\").replace('\"', '')\n",
    "            answer = review['answer'].strip().replace(\"'\", \"\").replace('\"', '')\n",
    "            comment = review['comment'].strip().replace(\"'\", \"\").replace('\"', '')\n",
    "            \n",
    "            # 데이터 정제 및 구조화\n",
    "            processed_data.append({\n",
    "                'question': question,\n",
    "                'answer': answer,\n",
    "                'comment': comment,\n",
    "                # instruction 형식으로 변환\n",
    "                'instruction': f\"다음 질문에 대한 답변을 작성해주세요:\\n{question}\",\n",
    "                # input/output 형식으로 변환\n",
    "                'input': question,\n",
    "                'output': answer,\n",
    "                'feedback': comment\n",
    "            })\n",
    "\n",
    "    # Dataset 객체로 변환\n",
    "    dataset = Dataset.from_list(processed_data)\n",
    "    \n",
    "    # 학습용 프롬프트 템플릿 적용\n",
    "    def apply_prompt_template(examples):\n",
    "        prompts = []\n",
    "        for i in range(len(examples['question'])):\n",
    "            prompt = f\"\"\"### 질문:\n",
    "{examples['question'][i]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### 모범 답변:\n",
    "{examples['answer'][i]}\n",
    "\n",
    "### 피드백:\n",
    "{examples['comment'][i]}\n",
    "<|endoftext|>\"\"\"\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        return {'text': prompts}\n",
    "\n",
    "    # 프롬프트 템플릿 적용\n",
    "    formatted_dataset = dataset.map(apply_prompt_template, batched=True)\n",
    "    \n",
    "    return formatted_dataset\n",
    "\n",
    "# 데이터셋 생성\n",
    "dataset = load_and_preprocess_data()\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(f\"데이터셋 크기: {len(dataset)}\")\n",
    "print(\"\\n데이터셋 예시:\")\n",
    "print(dataset[0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # 토크나이저의 패딩을 오른쪽으로 설정합니다.\n",
    "\n",
    "# SFTTrainer를 사용하여 모델 학습 설정\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # 학습할 모델\n",
    "    tokenizer=tokenizer,  # 토크나이저\n",
    "    train_dataset=dataset['train'],  # 학습 데이터셋\n",
    "    eval_dataset=dataset['train'],# 테디노트에서 추가된 eval dataset\n",
    "    dataset_text_field=\"text\",  # 데이터셋에서 텍스트 필드의 이름\n",
    "    dataset_num_proc=2,  # 데이터 처리에 사용할 프로세스 수\n",
    "    packing=False,  # 짧은 시퀀스에 대한 학습 속도를 5배 빠르게 할 수 있음\n",
    "    args=SFTConfig(\n",
    "        max_seq_length=7994,  # 최대 시퀀스 길이\n",
    "        per_device_train_batch_size=2,  # 각 디바이스당 훈련 배치 크기\n",
    "        gradient_accumulation_steps=4,  # 그래디언트 누적 단계\n",
    "        warmup_steps=5,  # 웜업 스텝 수\n",
    "        num_train_epochs=3,  # 훈련 에폭 수\n",
    "        max_steps=100,  # 최대 스텝 수 # 공식문서에는 60\n",
    "        logging_steps=1,  # logging 스텝 수\n",
    "        learning_rate=2e-4,  # 학습률\n",
    "        fp16=not torch.cuda.is_bf16_supported(),  # fp16 사용 여부, bf16이 지원되지 않는 경우에만 사용\n",
    "        bf16=torch.cuda.is_bf16_supported(),  # bf16 사용 여부, bf16이 지원되는 경우에만 사용\n",
    "        optim=\"adamw_8bit\",  # 최적화 알고리즘\n",
    "        weight_decay=0.01,  # 가중치 감소\n",
    "        lr_scheduler_type=\"cosine\",  # 학습률 스케줄러 유형 # 공식은 linear\n",
    "        seed=123,  # 랜덤 시드 # 공식은 3407\n",
    "        output_dir=\"outputs\",  # 출력 디렉토리\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title Training model\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer([\"'차세대 예지 보전 설비기술 도입' 21살 때, 삼성전자 화성캠퍼스 신설 현장에서 일하며 반도체 설비에 관한 관심을 키우게 되었습니다. 당시, 공장 내부 가스관 설치 현장에서 배관 시공을 간접적으로 경험하며 설비 유지 보수에 관심을 키우게 되었습니다. 아무리 완벽한 반도체 이론이 있더라도 원활한 설비 공급 없이는 제품을 구현할 수 없다는 점을 느꼈습니다. 삼성전자 반도체 연구소는 세계 최고의 기술을 연구하여 글로벌 메모리 반도체 1위 자리를 30년간 지켜내고 있습니다. 이런 설비기술의 선구자인 삼성전자에서 차세대 예지 보전 설비관리 특허보유라는 꿈을 이루고 싶습니다.\"], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 512, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
